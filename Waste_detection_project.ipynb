{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-NuY2BLAENb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.ops import box_iou"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "with zipfile.ZipFile('/Waste detection.v1i.createml.zip','r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')\n",
        "print('done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GdVh0YHFMyg",
        "outputId": "b2da9f9e-e97d-4e1b-c4f3-614cdae410a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install ipdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_ZLJelzClqJ",
        "outputId": "51f79a11-c1b4-4eee-d624-737f35f05986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipdb in /usr/local/lib/python3.10/dist-packages (0.13.13)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.10/dist-packages (from ipdb) (7.34.0)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from ipdb) (2.2.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.19.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb) (0.2.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Fonction de collation pour gérer les tailles variables dans les annotations.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    coords = []\n",
        "\n",
        "    for item in batch:\n",
        "        images.append(item[0])  # Image tensor\n",
        "        labels.append(item[1]['labels'])  # Labels\n",
        "        coords.append(item[1]['coords'])  # Bounding boxes\n",
        "\n",
        "    # Empile les images (taille fixe grâce à des transformations)\n",
        "    images = torch.stack(images, dim=0)\n",
        "\n",
        "    # Utilise des listes pour les labels et coords (tailles variables)\n",
        "    return images, {'labels': labels, 'coords': coords}\n"
      ],
      "metadata": {
        "id": "i2-dRJrLLPm7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.ops import box_iou\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "# Charger les annotations JSON\n",
        "def load_annotations(json_path):\n",
        "    with open(json_path, 'r') as f:\n",
        "        annotations = json.load(f)\n",
        "    return annotations\n",
        "\n",
        "\n",
        "# Dataset pour la détection d'objets\n",
        "class ObjectDetectionDataset(Dataset):\n",
        "    def __init__(self, annotations, images_dir, transform=None, label_map=None):\n",
        "        self.annotations = annotations\n",
        "        self.images_dir = images_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        if label_map is None:\n",
        "            self.label_map = self.build_label_map()\n",
        "        else:\n",
        "            self.label_map = label_map\n",
        "\n",
        "\n",
        "    def build_label_map(self):\n",
        "        labels = set()\n",
        "        for annotation in self.annotations:\n",
        "            for obj in annotation[\"annotations\"]:\n",
        "                labels.add(obj[\"label\"])\n",
        "        print(\"classes détectées dans le json: \", labels)\n",
        "        return {label: idx for idx, label in enumerate(sorted(labels))}\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        annotation = self.annotations[idx]\n",
        "        image_name = annotation[\"image\"]\n",
        "        image_path = os.path.join(self.images_dir, image_name)\n",
        "\n",
        "        # Charger l'image\n",
        "        image = cv2.imread(image_path)\n",
        "        print(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Charger les bounding boxes et labels\n",
        "        coords = []\n",
        "        labels = []\n",
        "        for obj in annotation[\"annotations\"]:\n",
        "            x, y = obj[\"coordinates\"][\"x\"], obj[\"coordinates\"][\"y\"]\n",
        "            w, h = obj[\"coordinates\"][\"width\"], obj[\"coordinates\"][\"height\"]\n",
        "            coords.append([x, y, w, h])\n",
        "            labels.append(self.label_map[obj[\"label\"]])\n",
        "\n",
        "        coords = torch.tensor(coords, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "\n",
        "        # Appliquer des transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return (image, {'coords': coords, 'labels': labels})\n",
        "\n",
        "# Préparer le transformateur pour les images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Resize((300, 300)),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "# Charger les annotations et initialiser le DataLoader\n",
        "annotations_file = \"/content/train/_annotations.createml.json\"  # Remplacez par le chemin correct\n",
        "images_dir = \"/content/train\"  # Remplacez par le dossier contenant les images\n",
        "\n",
        "annotations = load_annotations(annotations_file)\n",
        "label_map = {\"plastic\": 0, \"cardboard\": 1, \"glass\": 2, \"metal\": 3}\n",
        "dataset = ObjectDetectionDataset(\n",
        "    annotations,\n",
        "    images_dir,\n",
        "    transform=transform,\n",
        "    label_map=label_map)\n",
        "\n",
        "\n",
        "dataloader = DataLoader(dataset,\n",
        "                        batch_size=4,\n",
        "                        shuffle=True,\n",
        "                        collate_fn=custom_collate_fn)\n",
        "\n",
        "# Construire le modèle avec ResNet en backbone\n",
        "class DetectionModel(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(DetectionModel, self).__init__()\n",
        "        backbone = models.resnet50(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(backbone.children())[:-2])  # Supprime les couches FC\n",
        "\n",
        "        self.conv = nn.Conv2d(2048, 1024, kernel_size=1)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.classifier = nn.Linear(1024, num_classes)  # Classification\n",
        "        self.regressor = nn.Linear(1024, 4)  # Régression des bounding boxes\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.feature_extractor(x)\n",
        "        features = self.conv(features)\n",
        "        pooled_features = self.pool(features)\n",
        "        pooled_features = pooled_features.flatten(start_dim=1)\n",
        "\n",
        "\n",
        "        class_logits = self.classifier(pooled_features)\n",
        "        coords = self.regressor(pooled_features)\n",
        "        return class_logits, coords\n",
        "\n",
        "# Initialiser le modèle, la perte et l'optimiseur\n",
        "num_classes = len(label_map)\n",
        "\n",
        "model = DetectionModel(num_classes)\n",
        "\n",
        "criterion_class = nn.CrossEntropyLoss()\n",
        "criterion_coords = nn.SmoothL1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Entraîner le modèle\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, targets in dataloader:\n",
        "        #import ipdb; ipdb.set_trace()\n",
        "        images = images.to(device)\n",
        "        #labels = torch.cat([t['labels'] for t in targets]).to(device)\n",
        "        #coords = torch.cat([t['coords'] for t in targets]).to(device)\n",
        "\n",
        "        labels = [torch.tensor(t).to(device) for t in targets['labels']]\n",
        "        coords = [torch.tensor(t).to(device) for t in targets['coords']]\n",
        "\n",
        "        # Forward pass\n",
        "        class_logits, predicted_coords = model(images)\n",
        "\n",
        "        # Calcul des pertes\n",
        "        print(class_logits.shape)\n",
        "        num_classes = len(labels)\n",
        "        num_coords = len(coords)\n",
        "        print(num_classes)\n",
        "         # print(labels.shape)\n",
        "        print(predicted_coords.shape)\n",
        "        print(num_coords)\n",
        "\n",
        "\n",
        "        \"\"\" loss_class = criterion_class(class_logits, labels)\n",
        "        loss_coords = criterion_coords(predicted_coords, coords)\n",
        "        loss = loss_class + loss_coords\"\"\"\n",
        "\n",
        "\n",
        "        # Calcul des pertes pour chaque élément du batch\n",
        "        loss_class = 0\n",
        "        loss_coords = 0\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            # Calcul des pertes pour les classes\n",
        "            loss_class += criterion_class(class_logits[i], labels[i])\n",
        "\n",
        "            # Calcul des pertes pour les coordonnées\n",
        "            loss_coords += criterion_coords(predicted_coords[i], coords[i])\n",
        "\n",
        "        # Moyenne sur le batch\n",
        "        loss_class /= len(labels)\n",
        "        loss_coords /= len(coords)\n",
        "\n",
        "        # Perte totale\n",
        "        loss = loss_class + loss_coords\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(dataloader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "1VjdBQqSCqE3",
        "outputId": "b4422f78-b4b7-424b-d90c-2fd710367b48"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/train/IMG_9149_36_11zon_jpg.rf.3caa10538aab3290cb4e3ee56df626a0.jpg\n",
            "/content/train/Image-14-_15_11zon_jpeg.rf.ad0a16c36e2ae75371e414a6b9249693.jpg\n",
            "/content/train/IMG_9245_2_11zon_jpg.rf.e6a334fe161599eec702546d16026fa2.jpg\n",
            "/content/train/IMG_9334_78_11zon_jpg.rf.2f3eadb45bfea7f789deed0878ba78d6.jpg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-26be71d7df10>:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = [torch.tensor(t).to(device) for t in targets['labels']]\n",
            "<ipython-input-25-26be71d7df10>:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  coords = [torch.tensor(t).to(device) for t in targets['coords']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 4])\n",
            "4\n",
            "torch.Size([4, 4])\n",
            "4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "size mismatch (got input: [4], target: [1])",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-26be71d7df10>\u001b[0m in \u001b[0;36m<cell line: 138>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;31m# Calcul des pertes pour les classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mloss_class\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;31m# Calcul des pertes pour les coordonnées\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m         return F.cross_entropy(\n\u001b[0m\u001b[1;32m   1294\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3480\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: size mismatch (got input: [4], target: [1])"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#def draw_bounding_boxes(image, predicted_coords, labels, label_map):\n",
        "    plt.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
        "    for coord, label in zip(predicted_coords, labels):\n",
        "        x, y, w, h = coord\n",
        "        x_min, y_min = x - w / 2, y - h / 2\n",
        "        rect = patches.Rectangle(\n",
        "            (x_min, y_min), w, h,\n",
        "            linewidth=2, edgecolor='r', facecolor='none'\n",
        "        )\n",
        "        plt.gca().add_patch(rect)\n",
        "        plt.text(x_min, y_min, list(label_map.keys())[label.item()], color='r', fontsize=10)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Inférence et visualisation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, targets in dataloader:\n",
        "        images = torch.stack(images).to(device)\n",
        "        class_logits, predicted_coords = model(images)\n",
        "\n",
        "        for i in range(len(images)):\n",
        "            draw_bounding_boxes(images[i], predicted_coords[i].cpu(), torch.argmax(class_logits, dim=1).cpu(), label_map)\n",
        "        break"
      ],
      "metadata": {
        "id": "ysAeYy4EC4o-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}